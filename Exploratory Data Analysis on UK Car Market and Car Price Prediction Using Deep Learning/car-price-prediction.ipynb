{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I. Importing Data\n\nFirst and foremost, importing the libraries needed and all the data. We are doing this by using *numpy* and *pandas*.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:14:50.715273Z","iopub.execute_input":"2021-06-24T07:14:50.715836Z","iopub.status.idle":"2021-06-24T07:14:50.735232Z","shell.execute_reply.started":"2021-06-24T07:14:50.715688Z","shell.execute_reply":"2021-06-24T07:14:50.734044Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/used-car-dataset-ford-and-mercedes/cclass.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/unclean cclass.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/focus.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/audi.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/toyota.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/skoda.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/ford.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/vauxhall.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/bmw.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/vw.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/hyundi.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/unclean focus.csv\n/kaggle/input/used-car-dataset-ford-and-mercedes/merc.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see above, there are a lot of data from different files. Each file represent different brand. But, when you see the location of these files, you can see that the name of the files are also the brand. Then we just have to manipulate the location of the file, extract the name of the file to name the dataframe that later we will be using. ","metadata":{}},{"cell_type":"code","source":"sources = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        sources.append(os.path.join(dirname, filename))\n        \ncar_names = [j.split('/')[-1].split('.')[0] for j in sources]\ndf = {}\nfor c, s in zip(car_names, sources):\n    df[c] = pd.read_csv(s)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:14:52.415432Z","iopub.execute_input":"2021-06-24T07:14:52.416024Z","iopub.status.idle":"2021-06-24T07:14:52.784065Z","shell.execute_reply.started":"2021-06-24T07:14:52.415985Z","shell.execute_reply":"2021-06-24T07:14:52.783131Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"for n in car_names:\n    print(n)\n    print(df[n].columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:14:52.788425Z","iopub.execute_input":"2021-06-24T07:14:52.790458Z","iopub.status.idle":"2021-06-24T07:14:52.801910Z","shell.execute_reply.started":"2021-06-24T07:14:52.790408Z","shell.execute_reply":"2021-06-24T07:14:52.801065Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cclass\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType',\n       'engineSize'],\n      dtype='object')\nunclean cclass\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuel type',\n       'engine size', 'mileage2', 'fuel type2', 'engine size2', 'reference'],\n      dtype='object')\nfocus\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType',\n       'engineSize'],\n      dtype='object')\naudi\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\ntoyota\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\nskoda\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\nford\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\nvauxhall\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\nbmw\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\nvw\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\nhyundi\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType',\n       'tax(£)', 'mpg', 'engineSize'],\n      dtype='object')\nunclean focus\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuel type',\n       'engine size', 'mileage2', 'fuel type2', 'engine size2', 'reference'],\n      dtype='object')\nmerc\nIndex(['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax',\n       'mpg', 'engineSize'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see above, each dataframe has different name of columns. Even though they actually showed the similar features (*'fuelType'* and *'fuel type'* for example, they both told the data of what fuel a car used). Because of this, we need to change the name of some columns.","metadata":{}},{"cell_type":"code","source":"df['hyundi'] = df['hyundi'].rename(columns={'tax(£)':'tax'})\ndf['unclean cclass'] = df['unclean cclass'].rename(columns={'fuel type':'fuelType', 'engine size':'engineSize',\n                                                            'fuel type2':'fuelType2', 'engine size2':'engineSize2'})","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:14:52.981792Z","iopub.execute_input":"2021-06-24T07:14:52.982262Z","iopub.status.idle":"2021-06-24T07:14:52.996967Z","shell.execute_reply.started":"2021-06-24T07:14:52.982220Z","shell.execute_reply":"2021-06-24T07:14:52.995815Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## II. Exploratory Data Analysis\n\nNow, I want to find what features from a car that contribute to its cost the most. We can find this out by doing an ANOVA test. We would make the price as the target, and all other features as the data being analyzed. Here, we also excluded categorical features like *model*, *transmission* and *fuel type*. ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_regression\n\ntrain, test = df['audi'].drop(['price', 'model', 'transmission', 'fuelType'], axis=1), df['audi']['price']\nselector = SelectKBest(f_regression, k=3)\ntrain = selector.fit_transform(train, test)\n\nimportant_cols = []\nfor i in range(3):\n    for j in range(len(df['audi'].columns)):\n        if sum(train[:, i] == df['audi'].iloc[:, j]) == len(df['audi']):\n            important_cols.append(df['audi'].columns[j])\n            \nimportant_cols","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:14:53.531060Z","iopub.execute_input":"2021-06-24T07:14:53.531470Z","iopub.status.idle":"2021-06-24T07:14:55.035474Z","shell.execute_reply.started":"2021-06-24T07:14:53.531439Z","shell.execute_reply":"2021-06-24T07:14:55.034364Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['year', 'mpg', 'engineSize']"},"metadata":{}}]},{"cell_type":"markdown","source":"From the result above, we can see that **year, miles per gallon** and **engine size** are the most important features contributing to price respectively. This analysis by the way, only the analysis on Audi car. But we can assume that the result would be the same if we analyzed another brand. ","metadata":{}},{"cell_type":"code","source":"df['toyota'].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:14:55.037070Z","iopub.execute_input":"2021-06-24T07:14:55.037375Z","iopub.status.idle":"2021-06-24T07:14:55.059020Z","shell.execute_reply.started":"2021-06-24T07:14:55.037347Z","shell.execute_reply":"2021-06-24T07:14:55.057924Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   model  year  price transmission  mileage fuelType  tax   mpg  engineSize\n0   GT86  2016  16000       Manual    24089   Petrol  265  36.2         2.0\n1   GT86  2017  15995       Manual    18615   Petrol  145  36.2         2.0\n2   GT86  2015  13998       Manual    27469   Petrol  265  36.2         2.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>year</th>\n      <th>price</th>\n      <th>transmission</th>\n      <th>mileage</th>\n      <th>fuelType</th>\n      <th>tax</th>\n      <th>mpg</th>\n      <th>engineSize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GT86</td>\n      <td>2016</td>\n      <td>16000</td>\n      <td>Manual</td>\n      <td>24089</td>\n      <td>Petrol</td>\n      <td>265</td>\n      <td>36.2</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GT86</td>\n      <td>2017</td>\n      <td>15995</td>\n      <td>Manual</td>\n      <td>18615</td>\n      <td>Petrol</td>\n      <td>145</td>\n      <td>36.2</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GT86</td>\n      <td>2015</td>\n      <td>13998</td>\n      <td>Manual</td>\n      <td>27469</td>\n      <td>Petrol</td>\n      <td>265</td>\n      <td>36.2</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# III. Model Building","metadata":{}},{"cell_type":"markdown","source":"Now, we will make the model that can predict the price of a car. The features of this model would be:\n\n**1.** Brand\n\n**2.** Year being bought\n\n**3.** Model\n\n**4.** Transmission\n\n**5.** Mileage\n\n**6.** Fuel Type\n\n**7.** Miles per gallon\n\n**8.** Engine Size\n\nTo make the model more accurate, **we would just make one model for each brand**. These brands have their own inner value that is hard to measure in numbers (we could, but we wouldn't do that here). Beside, making a model for each brand is also an attempt to discard the effect of model's luxury that we can't calculate. \n\nWe also built this by using functions. Later on, when we need to make a new model on new brand, we just need to call this functions. Or even make a loop so we can get all models at once. ","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef made_up_encoder(dx, param):\n    a = dx[[param, 'price']].groupby(param).mean().sort_values('price', ascending='True').index\n    return a\n\ndef convert_to_num(dx, param, mu):\n    a = []\n    for i in dx[param]:\n        for j, k in enumerate(mu):\n            if i == k: \n                a.append(j)\n                break\n    return a\n\ndef made_up_transformer(ds, params):\n    for p in params:\n        a = made_up_encoder(ds, p)\n        ds[p] = convert_to_num(ds, p, a)\n    return ds\n\ndef the_model(x_train, y_train):\n    model = Sequential()\n    model.add(Dense(10, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(5, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='relu'))\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    model.fit(x_train, y_train, epochs=15)\n    return model\n\ndef process_for_model(dv, params):\n    cols = dv.columns\n    dv_new = pd.DataFrame(made_up_transformer(dv, params))\n    dv_new = pd.DataFrame(MinMaxScaler().fit_transform(dv_new), columns=cols)\n    \n    x_ = dv_new.drop(['price'], axis=1)\n    y_ = dv_new.price\n    \n    x_train, x_valid, y_train, y_valid = train_test_split(x_, y_, test_size=0.2, random_state=42)\n    \n    model_car = the_model(x_train, y_train)\n    \n    return model_car, x_train, x_valid, y_train, y_valid","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:14:55.333006Z","iopub.execute_input":"2021-06-24T07:14:55.333466Z","iopub.status.idle":"2021-06-24T07:15:01.842287Z","shell.execute_reply.started":"2021-06-24T07:14:55.333431Z","shell.execute_reply":"2021-06-24T07:15:01.841001Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Here we use Artificial Neural Network as the model.**\n\nThere are some features that are categorical. We can just use Label Encoder. But that's not what we would do here. We realize that this categorical data also contribute to price. There are certain models that are expensive than the other just because of the model. Same thing goes to fuel type. \n\nSo, we decided to do something like Label Encoder but the value that has higher average of price would be labeled higher (BMW X7 would labeled higher than BMW X5 for example).","metadata":{}},{"cell_type":"code","source":"model = {}\nx_train = {}\nx_valid = {}\ny_train = {}\ny_valid = {}\n\ncar_brands = ['audi', 'toyota', 'skoda', 'ford', 'vauxhall', 'bmw', 'vw', 'hyundi', 'merc']\n\nfor c in car_brands:\n    model[c], x_train[c], x_valid[c], y_train[c], y_valid[c] = process_for_model(df[c],\n                                                                                 ['model', 'transmission', 'fuelType'])","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:15:01.848894Z","iopub.execute_input":"2021-06-24T07:15:01.849360Z","iopub.status.idle":"2021-06-24T07:15:52.777988Z","shell.execute_reply.started":"2021-06-24T07:15:01.849308Z","shell.execute_reply":"2021-06-24T07:15:52.776671Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/15\n267/267 [==============================] - 1s 1ms/step - loss: 0.0266 - mae: 0.1368\nEpoch 2/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0217 - mae: 0.1190\nEpoch 3/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0053 - mae: 0.0520\nEpoch 4/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0035 - mae: 0.0384\nEpoch 5/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0027 - mae: 0.0328\nEpoch 6/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0022 - mae: 0.0314\nEpoch 7/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0304\nEpoch 8/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0022 - mae: 0.0302\nEpoch 9/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0303\nEpoch 10/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0291\nEpoch 11/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0290\nEpoch 12/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0283\nEpoch 13/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0281\nEpoch 14/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0016 - mae: 0.0269\nEpoch 15/15\n267/267 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0277\nEpoch 1/15\n169/169 [==============================] - 1s 1ms/step - loss: 0.0414 - mae: 0.1690\nEpoch 2/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0129 - mae: 0.0756\nEpoch 3/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0097 - mae: 0.0679\nEpoch 4/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0085 - mae: 0.0647\nEpoch 5/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0073 - mae: 0.0578\nEpoch 6/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0066 - mae: 0.0563\nEpoch 7/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0069 - mae: 0.0557\nEpoch 8/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0063 - mae: 0.0540\nEpoch 9/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0064 - mae: 0.0538\nEpoch 10/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0064 - mae: 0.0551\nEpoch 11/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0059 - mae: 0.0515\nEpoch 12/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0055 - mae: 0.0502\nEpoch 13/15\n169/169 [==============================] - 0s 2ms/step - loss: 0.0053 - mae: 0.0504\nEpoch 14/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0058 - mae: 0.0507\nEpoch 15/15\n169/169 [==============================] - 0s 1ms/step - loss: 0.0053 - mae: 0.0498\nEpoch 1/15\n157/157 [==============================] - 1s 1ms/step - loss: 0.0178 - mae: 0.1050\nEpoch 2/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0072 - mae: 0.0616\nEpoch 3/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0039 - mae: 0.0460\nEpoch 4/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0031 - mae: 0.0398\nEpoch 5/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0026 - mae: 0.0367\nEpoch 6/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0024 - mae: 0.0355\nEpoch 7/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0334\nEpoch 8/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0020 - mae: 0.0324\nEpoch 9/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0322\nEpoch 10/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0019 - mae: 0.0307\nEpoch 11/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0017 - mae: 0.0293\nEpoch 12/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0297\nEpoch 13/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0302\nEpoch 14/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0296\nEpoch 15/15\n157/157 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0287\nEpoch 1/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0194 - mae: 0.1058\nEpoch 2/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0056 - mae: 0.0538\nEpoch 3/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0040 - mae: 0.0471\nEpoch 4/15\n450/450 [==============================] - 0s 1ms/step - loss: 0.0033 - mae: 0.0419\nEpoch 5/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0031 - mae: 0.0395\nEpoch 6/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0027 - mae: 0.0384\nEpoch 7/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0028 - mae: 0.0379\nEpoch 8/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0025 - mae: 0.0370\nEpoch 9/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0025 - mae: 0.0370\nEpoch 10/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0028 - mae: 0.0378\nEpoch 11/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0025 - mae: 0.0362\nEpoch 12/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0025 - mae: 0.0366\nEpoch 13/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0025 - mae: 0.0360\nEpoch 14/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0024 - mae: 0.0355\nEpoch 15/15\n450/450 [==============================] - 1s 1ms/step - loss: 0.0025 - mae: 0.0352\nEpoch 1/15\n341/341 [==============================] - 1s 1ms/step - loss: 0.0216 - mae: 0.1216\nEpoch 2/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0037 - mae: 0.0465\nEpoch 3/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0030 - mae: 0.0408\nEpoch 4/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0027 - mae: 0.0383\nEpoch 5/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0025 - mae: 0.0364\nEpoch 6/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0025 - mae: 0.0363\nEpoch 7/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0024 - mae: 0.0359\nEpoch 8/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0024 - mae: 0.0354\nEpoch 9/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0351\nEpoch 10/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0350\nEpoch 11/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0348\nEpoch 12/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0349\nEpoch 13/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0022 - mae: 0.0343\nEpoch 14/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0348\nEpoch 15/15\n341/341 [==============================] - 0s 1ms/step - loss: 0.0022 - mae: 0.0346\nEpoch 1/15\n270/270 [==============================] - 1s 1ms/step - loss: 0.0302 - mae: 0.1418\nEpoch 2/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0065 - mae: 0.0582\nEpoch 3/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0060 - mae: 0.0548\nEpoch 4/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0049 - mae: 0.0509\nEpoch 5/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0050 - mae: 0.0504\nEpoch 6/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0047 - mae: 0.0486\nEpoch 7/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0041 - mae: 0.0464\nEpoch 8/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0039 - mae: 0.0446\nEpoch 9/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0037 - mae: 0.0439\nEpoch 10/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0036 - mae: 0.0430\nEpoch 11/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0036 - mae: 0.0413\nEpoch 12/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0032 - mae: 0.0400\nEpoch 13/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0032 - mae: 0.0395\nEpoch 14/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0030 - mae: 0.0390\nEpoch 15/15\n270/270 [==============================] - 0s 1ms/step - loss: 0.0030 - mae: 0.0378\nEpoch 1/15\n379/379 [==============================] - 1s 1ms/step - loss: 0.0237 - mae: 0.1188\nEpoch 2/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0098 - mae: 0.0750\nEpoch 3/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0063 - mae: 0.0585\nEpoch 4/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0058 - mae: 0.0544\nEpoch 5/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0049 - mae: 0.0508\nEpoch 6/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0048 - mae: 0.0489\nEpoch 7/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0043 - mae: 0.0472\nEpoch 8/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0043 - mae: 0.0463\nEpoch 9/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0038 - mae: 0.0446\nEpoch 10/15\n379/379 [==============================] - 1s 1ms/step - loss: 0.0040 - mae: 0.0455\nEpoch 11/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0040 - mae: 0.0447\nEpoch 12/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0040 - mae: 0.0442\nEpoch 13/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0035 - mae: 0.0420\nEpoch 14/15\n379/379 [==============================] - 1s 1ms/step - loss: 0.0035 - mae: 0.0415\nEpoch 15/15\n379/379 [==============================] - 0s 1ms/step - loss: 0.0037 - mae: 0.0418\nEpoch 1/15\n122/122 [==============================] - 1s 1ms/step - loss: 0.0129 - mae: 0.0945\nEpoch 2/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0039 - mae: 0.0446\nEpoch 3/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0027 - mae: 0.0368\nEpoch 4/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0024 - mae: 0.0354\nEpoch 5/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0025 - mae: 0.0354\nEpoch 6/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0331\nEpoch 7/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0020 - mae: 0.0321\nEpoch 8/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0315\nEpoch 9/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0309\nEpoch 10/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0017 - mae: 0.0302\nEpoch 11/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0016 - mae: 0.0294\nEpoch 12/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0018 - mae: 0.0303A: 0s - loss: 0.0018 - mae: 0.0\nEpoch 13/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0016 - mae: 0.0294\nEpoch 14/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0016 - mae: 0.0293\nEpoch 15/15\n122/122 [==============================] - 0s 1ms/step - loss: 0.0016 - mae: 0.0284\nEpoch 1/15\n328/328 [==============================] - 1s 1ms/step - loss: 0.0155 - mae: 0.0980\nEpoch 2/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0062 - mae: 0.0528\nEpoch 3/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0049 - mae: 0.0490\nEpoch 4/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0048 - mae: 0.0482\nEpoch 5/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0043 - mae: 0.0437\nEpoch 6/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0038 - mae: 0.0399\nEpoch 7/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0029 - mae: 0.0340\nEpoch 8/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0311\nEpoch 9/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0294\nEpoch 10/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0024 - mae: 0.0303\nEpoch 11/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0300\nEpoch 12/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0296\nEpoch 13/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0021 - mae: 0.0287\nEpoch 14/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0288\nEpoch 15/15\n328/328 [==============================] - 0s 1ms/step - loss: 0.0023 - mae: 0.0286\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\nscaler_price = {}\ny_predict = {}\ny_predict_conv = {}\ny_valid_conv = {}\nthe_errors = np.zeros([len(car_brands), 2])\n\nfor c in car_brands:\n    scaler_price[c] = MinMaxScaler().fit(np.array(df[c].price).reshape(-1, 1))\n\nfor i, c in enumerate(car_brands):\n    y_predict[c] = model[c].predict(x_valid[c])\n    \n    y_predict_conv[c] = scaler_price[c].inverse_transform(y_predict[c])\n    y_valid_conv[c] = scaler_price[c].inverse_transform(np.array(y_valid[c]).reshape(-1, 1))\n    \n    the_errors[i, 0] = mean_absolute_error(y_predict[c], y_valid[c])\n    the_errors[i, 1] = mean_absolute_error(y_predict_conv[c], y_valid_conv[c])\n    \ndf_errors = pd.DataFrame(the_errors, index=car_brands, columns=['MAE on scale', 'MAE on real price'])\ndf_errors","metadata":{"execution":{"iopub.status.busy":"2021-06-24T07:15:52.780071Z","iopub.execute_input":"2021-06-24T07:15:52.780559Z","iopub.status.idle":"2021-06-24T07:15:54.015420Z","shell.execute_reply.started":"2021-06-24T07:15:52.780505Z","shell.execute_reply":"2021-06-24T07:15:54.014424Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"          MAE on scale  MAE on real price\naudi          0.022325        3203.806726\ntoyota        0.040610        2401.865279\nskoda         0.032518        2955.204763\nford          0.028857        1572.685446\nvauxhall      0.027941        1453.999743\nbmw           0.033312        4072.568183\nvw            0.034928        2413.320569\nhyundi        0.024391        2214.743880\nmerc          0.025133        4004.989668","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAE on scale</th>\n      <th>MAE on real price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>audi</th>\n      <td>0.022325</td>\n      <td>3203.806726</td>\n    </tr>\n    <tr>\n      <th>toyota</th>\n      <td>0.040610</td>\n      <td>2401.865279</td>\n    </tr>\n    <tr>\n      <th>skoda</th>\n      <td>0.032518</td>\n      <td>2955.204763</td>\n    </tr>\n    <tr>\n      <th>ford</th>\n      <td>0.028857</td>\n      <td>1572.685446</td>\n    </tr>\n    <tr>\n      <th>vauxhall</th>\n      <td>0.027941</td>\n      <td>1453.999743</td>\n    </tr>\n    <tr>\n      <th>bmw</th>\n      <td>0.033312</td>\n      <td>4072.568183</td>\n    </tr>\n    <tr>\n      <th>vw</th>\n      <td>0.034928</td>\n      <td>2413.320569</td>\n    </tr>\n    <tr>\n      <th>hyundi</th>\n      <td>0.024391</td>\n      <td>2214.743880</td>\n    </tr>\n    <tr>\n      <th>merc</th>\n      <td>0.025133</td>\n      <td>4004.989668</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see from the result above, that Hyundi is a car brand that our model has the worst prediction on (Hyundi is not the right name, we all know that it's supposed to be Hyundai but whatever).The model predict on average the price of a car can be 11,268 pounds higher or lower, which oviously not a good number.\n\nBut the other model is quiet satisfying. They don't have an error that are huge. ","metadata":{}}]}