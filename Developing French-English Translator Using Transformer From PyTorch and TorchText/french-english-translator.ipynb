{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I. What We Will Do\nHere, I wanted to try to make my own french translator. Me, obviously are not a french speaker but I wanted to know how to speak it while using Google Translate is too common these days. This made decide to make a new one. Well, is it going to be comparable with Google Translate? Only time will tell. ","metadata":{}},{"cell_type":"markdown","source":"# II. How We Will Do It\n\nIn general, I will use a feature that is so-called Transformer. This is like a state-of-art technology in Natural Language Processing right now, which is also used by Google to make their translator. Then the rest of the process is just like any other Machine Learning project. Obtain the data, clean them off, find the best architecture and train them. Look how well it performed, and if we think it's not good enough, then we can fix some of the issues during the whole process or maybe start doing that again from the start. Simple isn't it?\n\nBut anyway, we get to the point. How are we going to do that? This is what we will do:\n\n1. Insert the Data\n2. Building Tokenizers and Vocabularies\n3. Making Dataloaders\n4. Building Transformer\n5. Training and Evaluation\n6. Translating","metadata":{}},{"cell_type":"markdown","source":"# III. Build 'Em\n## III.1. Insert the Data\nThis is what we will do to insert the data to this notebook.\n1. Upload the data, which is the french text and its english translation. I used *pandas* like always.\n2. Take some of the data, or use all of them if you knew you have the required computational power.\n3. Transform it to Dataset so PyTorch can use them later on as DataLoader.","metadata":{}},{"cell_type":"code","source":"!python -m spacy download fr_core_news_sm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport dask.dataframe as dd\nfrom torch.utils.data import Dataset, DataLoader\nimport spacy\nimport torch.nn as nn\nfrom torchtext.vocab import vocab, FastText\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-11-20T06:37:29.633871Z","iopub.execute_input":"2021-11-20T06:37:29.634447Z","iopub.status.idle":"2021-11-20T06:37:39.847671Z","shell.execute_reply.started":"2021-11-20T06:37:29.634353Z","shell.execute_reply":"2021-11-20T06:37:39.846976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dd_ = dd.read_csv('/kaggle/input/en-fr-translation-dataset/en-fr.csv')\ndd = pd.DataFrame(dd_.head(100000))\n\nclass words_dataset(Dataset):\n    def __init__(self, text):\n        self.text = text\n    def __len__(self):\n        return len(self.text)\n    def __getitem__(self, idx):\n        return self.text[idx]\n    \nen_dataset = words_dataset(dd['en'])\nfr_dataset = words_dataset(dd['fr'])","metadata":{"execution":{"iopub.status.busy":"2021-11-17T05:38:47.963048Z","iopub.execute_input":"2021-11-17T05:38:47.963287Z","iopub.status.idle":"2021-11-17T05:38:51.365832Z","shell.execute_reply.started":"2021-11-17T05:38:47.963255Z","shell.execute_reply":"2021-11-17T05:38:51.365081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## III.2. Building Tokenizers and Vocabularies\nThis is what we will do:\n1. Make the tokenizers for each language.\n2. Make vocabularies for each language.\n3. Insert special tokens to the vocabularies","metadata":{}},{"cell_type":"code","source":"en_spacy = spacy.load('en_core_web_sm')\nfr_spacy = spacy.load('fr_core_news_sm')\n\nen_tokenizer = lambda x: [y.text for y in en_spacy(str(x))]\nfr_tokenizer = lambda x: [y.text for y in fr_spacy(str(x))]\n\nen_fasttext = FastText(language='en')\nfr_fasttext = FastText(language='fr')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T05:38:51.367945Z","iopub.execute_input":"2021-11-17T05:38:51.3682Z","iopub.status.idle":"2021-11-17T06:01:05.240825Z","shell.execute_reply.started":"2021-11-17T05:38:51.368166Z","shell.execute_reply":"2021-11-17T06:01:05.237314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_vocab = vocab(en_fasttext.stoi)\nfr_vocab = vocab(fr_fasttext.stoi)\n\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n\nfor i, s in zip(range(4), special_symbols):\n    en_vocab.insert_token(s, i)\n    fr_vocab.insert_token(s, i)\n    \nen_vocab.set_default_index(en_vocab['<unk>'])\nfr_vocab.set_default_index(fr_vocab['<unk>'])","metadata":{"execution":{"iopub.status.busy":"2021-11-17T06:01:05.245664Z","iopub.execute_input":"2021-11-17T06:01:05.246566Z","iopub.status.idle":"2021-11-17T06:01:09.4494Z","shell.execute_reply.started":"2021-11-17T06:01:05.246521Z","shell.execute_reply":"2021-11-17T06:01:09.448623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## III.3. Making Dataloaders\nThis is what we will do:\n1. Split the data for training and evaluation\n2. Make *collate_fn* function\n3. Make the dataloaders","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\npad_idx = 1\nbos_idx = 2\neos_idx = 3\n\ndef collate_batch_en(batch):\n    batch_process = []\n    for text in batch:\n        process_1 = en_vocab(en_tokenizer(text))\n        process_2 = torch.cat((torch.tensor([bos_idx]), \n                               torch.tensor(process_1), \n                               torch.tensor([eos_idx])))\n        batch_process.append(process_2)\n    output = pad_sequence(batch_process, padding_value=pad_idx)\n    return output\n\ndef collate_batch_fr(batch):\n    batch_process = []\n    for text in batch:\n        process_1 = fr_vocab(fr_tokenizer(text))\n        process_2 = torch.cat((torch.tensor([bos_idx]), \n                               torch.tensor(process_1), \n                               torch.tensor([eos_idx])))\n        batch_process.append(process_2)\n    output = pad_sequence(batch_process, padding_value=pad_idx)\n    return output","metadata":{"execution":{"iopub.status.busy":"2021-11-17T06:01:09.450936Z","iopub.execute_input":"2021-11-17T06:01:09.451201Z","iopub.status.idle":"2021-11-17T06:01:09.462642Z","shell.execute_reply.started":"2021-11-17T06:01:09.451166Z","shell.execute_reply":"2021-11-17T06:01:09.461836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.dataset import random_split\n\nbatch_size = 128\nnum_train = int(len(en_dataset) * 0.95)\nnum_valid = len(en_dataset) - num_train\n\nen_train, en_valid = random_split(en_dataset, [num_train, num_valid])\nfr_train, fr_valid = random_split(fr_dataset, [num_train, num_valid])\n\ntrain_dataloader = {x: DataLoader(z, batch_size=batch_size, collate_fn=fn) \n                    for x, z, fn in zip(['en', 'fr'], [en_train, fr_train], [collate_batch_en, collate_batch_fr])}\nvalid_dataloader = {x: DataLoader(z, batch_size=batch_size, collate_fn=fn) \n                    for x, z, fn in zip(['en', 'fr'], [en_valid, fr_valid], [collate_batch_en, collate_batch_fr])}\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T06:01:09.465657Z","iopub.execute_input":"2021-11-17T06:01:09.465915Z","iopub.status.idle":"2021-11-17T06:01:09.486788Z","shell.execute_reply.started":"2021-11-17T06:01:09.465851Z","shell.execute_reply":"2021-11-17T06:01:09.486037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## III.4. Building Transformer\n1. Make function to generate masks\n2. Make object for Positional Encoding\n3. Make object for transformer","metadata":{}},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_padding_mask = (src == pad_idx).transpose(0, 1).to(device)\n    tgt_padding_mask = (tgt == pad_idx).transpose(0, 1).to(device)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask","metadata":{"execution":{"iopub.status.busy":"2021-11-17T06:01:09.48805Z","iopub.execute_input":"2021-11-17T06:01:09.488849Z","iopub.status.idle":"2021-11-17T06:01:09.559704Z","shell.execute_reply.started":"2021-11-17T06:01:09.488809Z","shell.execute_reply":"2021-11-17T06:01:09.558989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch import Tensor\nimport math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size:int, dropout:float, maxlen:int=5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n    def forward(self, token_embedding:Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n    \nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self, num_encoder_layers:int, num_decoder_layers:int, emb_size:int,\n                 nhead:int, src_vocab_size:int, tgt_vocab_size:int, dim_feedforward:int, dropout:float=0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers,\n                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n                                          dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = nn.Embedding.from_pretrained(fr_fasttext.vectors, freeze=True)\n        self.tgt_tok_emb = nn.Embedding.from_pretrained(en_fasttext.vectors, freeze=True)\n        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n        self.emb_size = emb_size\n    def forward(self, src:Tensor, tgt:Tensor, src_mask:Tensor, tgt_mask:Tensor,\n                src_padding_mask:Tensor, tgt_padding_mask:Tensor, memory_key_padding_mask:Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src) * math.sqrt(self.emb_size))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt) * math.sqrt(self.emb_size))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n    def encode(self, src:Tensor, src_mask:Tensor):\n        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n    def decode(self, tgt:Tensor, memory:Tensor, tgt_mask:Tensor):\n        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T06:01:09.561027Z","iopub.execute_input":"2021-11-17T06:01:09.561571Z","iopub.status.idle":"2021-11-17T06:01:09.580878Z","shell.execute_reply.started":"2021-11-17T06:01:09.561529Z","shell.execute_reply":"2021-11-17T06:01:09.580024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## III.5. Training and Evaluation\nThis is what we will do on this part:\n1. Make function for training\n2. Make function for evaluation\n3. Initialize the model and its hyperparameter\n4. Train and evaluate the model","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, src_dataloader, tgt_dataloader):\n    model.train()\n    losses = 0\n    \n    for src, tgt in zip(src_dataloader, tgt_dataloader):\n        src = src.to(device)\n        tgt = tgt.to(device)\n        tgt_input = tgt[:-1, :]\n        \n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n        optimizer.zero_grad()\n        \n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n        \n        optimizer.step()\n        losses += loss.item()\n        \n    return losses / len(src_dataloader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, src_dataloader, tgt_dataloader):\n    model.eval()\n    losses = 0\n    \n    for src, tgt in zip(src_dataloader, tgt_dataloader):\n        src = src.to(device)\n        tgt = tgt.to(device)\n        tgt_input = tgt[:-1, :]\n        \n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n        \n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n    \n    return losses / len(src_dataloader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src_vocab_size = fr_fasttext.vectors.shape[0]\ntgt_vocab_size = en_fasttext.vectors.shape[0]\nemb_size = fr_fasttext.vectors.shape[1] \nnhead = 5\nffn_hid_dim = emb_size\nbatch_size = 128\nnum_encoder_layers = 3\nnum_decoder_layers = 3\n\ntransformer = Seq2SeqTransformer(num_encoder_layers, num_decoder_layers, emb_size, nhead,\n                                 src_vocab_size, tgt_vocab_size, ffn_hid_dim)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n        \ntransformer = transformer.to(device)\nloss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\noptimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nnum_epochs = 20\nfor epoch in range(1, num_epochs + 1):\n    start_time = time.time()\n    train_loss = train_epoch(transformer, train_dataloader['fr'], train_dataloader['en'])\n    end_time = time.time()\n    eval_loss = evaluate(transformer, valid_dataloader['fr'], valid_dataloader['en'])\n    \n    elapsed = end_time - train_time\n    elapsed_time = '{} m and {:.2f} s'.format(elapsed // 60, elapsed % 60)\n    \n    print('Epoch : {}\\tTrain Loss : {:.3f}\\tEval Loss : {:.3f}'.format(elapsed_time, train_loss, eval_loss))","metadata":{"execution":{"iopub.status.busy":"2021-11-17T06:21:09.128498Z","iopub.execute_input":"2021-11-17T06:21:09.129051Z","iopub.status.idle":"2021-11-17T06:21:09.132655Z","shell.execute_reply.started":"2021-11-17T06:21:09.129013Z","shell.execute_reply":"2021-11-17T06:21:09.131929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## III.6. Translating","metadata":{}},{"cell_type":"code","source":"def transform_text_fr(text:str):\n    process = fr_vocab(fr_tokenizer(text))\n    process = torch.cat((torch.tensor([bos_idx]), \n                         torch.tensor(process), \n                         torch.tensor([eos_idx])))\n    return process","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(device)\n    src_mask = src_mask.to(device)\n    \n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n    \n    for i in range(max_len - 1):\n        memory = memory.to(device)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(device)\n        \n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        \n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n        \n        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        \n        if next_word == eos_idx:\n            break\n            \n    return ys","metadata":{"execution":{"iopub.status.busy":"2021-11-17T06:21:26.889828Z","iopub.execute_input":"2021-11-17T06:21:26.890361Z","iopub.status.idle":"2021-11-17T06:21:26.894186Z","shell.execute_reply.started":"2021-11-17T06:21:26.890323Z","shell.execute_reply":"2021-11-17T06:21:26.893289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate(model, src_sentence:str):\n    model.eval()\n    src = text_transform_fr(src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(model, src, src_mask, max_len=(num_tokens + 5), start_symbol=bos_idx).flatten()\n    return ' '.join(en_vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace('<bos>', '')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use the translation machine, you can use the function: \n\n*translate(transformer, #your_text)*","metadata":{}}]}